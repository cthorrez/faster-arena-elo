{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyl5Vil7HRzd"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, we present data analysis on Chatbot Arena data collected from https://arena.lmsys.org.\n",
        "\n",
        "We explain different Elo calculation methods (online Elo and MLE Elo, also known as Bradley-Terry model) for model ranking.\n",
        "\n",
        "To view the latest leaderboard, see https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C5H_wlbqGwCJ"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from tqdm import tqdm\n",
        "pd.options.display.float_format = '{:.2f}'.format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ0G_G-sHwm3"
      },
      "source": [
        "# Obtaining and Cleaning the Tournament Data\n",
        "We are hosting the initial tournament results as a JSON file on Google Drive. We use the `gdown` function to download the data. The data contains all the battels and voting results collected for ranking models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EZvUIOhVZD27"
      },
      "outputs": [],
      "source": [
        "# we use the latest data\n",
        "# url = \"https://storage.googleapis.com/arena_external_data/public/clean_battle_20240814_public.json\"\n",
        "# response = requests.get(url)\n",
        "\n",
        "# with open('local_file_name.json', 'wb') as file:\n",
        "#     file.write(response.content)\n",
        "\n",
        "# load the JSON data from the local file\n",
        "with open('local_file_name.json', 'r') as file:\n",
        "    battles = pd.read_json(file).sort_values(ascending=True, by=[\"tstamp\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "-0rg26TQxFQv",
        "outputId": "42082cb3-18ae-4475-cb4c-fb93feea2000"
      },
      "outputs": [],
      "source": [
        "battles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2QC3dU2A8v2",
        "outputId": "93a59952-4143-46c2-f610-d2e68ab35464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before dedup:  1799991\n",
            "After dedup:  1670250\n"
          ]
        }
      ],
      "source": [
        "# we use anony battles only for leaderboard\n",
        "battles = battles[battles[\"anony\"] == True]\n",
        "\n",
        "# we de-duplicate top 0.1% redudant prompts\n",
        "# see https://lmsys.org/blog/2024-05-17-category-hard/#note-enhancing-quality-through-de-duplication\n",
        "print(\"Before dedup: \", len(battles))\n",
        "battles = battles[battles[\"dedup_tag\"].apply(lambda x: x.get(\"sampled\", False))]\n",
        "print(\"After dedup: \", len(battles))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IdpT27Q8IE_"
      },
      "source": [
        "# Exploratory Analysis\n",
        "\n",
        "Before computing the Elo ratings, we first conduct some basic exploratory analysis to highlight a few key properties and caveates with this data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioFGqU_O8a1f"
      },
      "source": [
        "## Statistics\n",
        "\n",
        "We allowed the user to declare a tie between the pairs of models.  To collect additional data, later in the tournament we also allowed the user to declare a tie in which both models were bad.  There were a significant portion of tied outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "8mcfYn7dIPVl",
        "outputId": "853692a9-a30a-4217-d4db-aa2d558c7ee6"
      },
      "outputs": [],
      "source": [
        "fig = px.bar(battles[\"winner\"].value_counts(),\n",
        "             title=\"Counts of Battle Outcomes\", text_auto=True, height=400)\n",
        "fig.update_layout(xaxis_title=\"Battle Outcome\", yaxis_title=\"Count\",\n",
        "                  showlegend=False)\n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "csZ2iiW4SDN3"
      },
      "outputs": [],
      "source": [
        "battles_no_ties = battles[~battles[\"winner\"].str.contains(\"tie\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWtkYOdR1LQl"
      },
      "source": [
        "## Non-uniform Model Frequency\n",
        "\n",
        "The model frequency is not uniform because of the follwoing reasons:\n",
        "- Several different matching and sampling algorithms were used. We employed uniform sampling as well as weighted sampling methods, which assign greater weights to better models.\n",
        "- Some new models were added later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "q4vYYIjlzbLk",
        "outputId": "0a296268-7e4d-47a8-8df5-de5aea9fa01c"
      },
      "outputs": [],
      "source": [
        "fig = px.bar(pd.concat([battles[\"model_a\"], battles[\"model_b\"]]).value_counts(),\n",
        "             title=\"Battle Count for Each Model\", text_auto=True)\n",
        "fig.update_layout(xaxis_title=\"model\", yaxis_title=\"Battle Count\", height=400,\n",
        "                  showlegend=False)\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wCkDV405usE"
      },
      "source": [
        "We examing the number of pairings for each combination of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "OyC00Fyv10Bs",
        "outputId": "d2bcfc0c-f20b-4a20-e6f4-7b355fef7df1"
      },
      "outputs": [],
      "source": [
        "def visualize_battle_count(battles, title, show_num_models=30):\n",
        "    ptbl = pd.pivot_table(battles, index=\"model_a\", columns=\"model_b\", aggfunc=\"size\",\n",
        "                          fill_value=0)\n",
        "    battle_counts = ptbl + ptbl.T\n",
        "    ordering = battle_counts.sum().sort_values(ascending=False).index\n",
        "    ordering = ordering[:show_num_models]\n",
        "    fig = px.imshow(battle_counts.loc[ordering, ordering],\n",
        "                    title=title, text_auto=True)\n",
        "    fig.update_layout(xaxis_title=\"Model B\",\n",
        "                      yaxis_title=\"Model A\",\n",
        "                      xaxis_side=\"top\", height=800, width=800,\n",
        "                      title_y=0.07, title_x=0.5,\n",
        "                      font=dict(size=10))\n",
        "    fig.update_traces(hovertemplate=\n",
        "                      \"Model A: %{y}<br>Model B: %{x}<br>Count: %{z}<extra></extra>\")\n",
        "    return fig\n",
        "\n",
        "fig = visualize_battle_count(battles, title=\"Battle Count of Each Combination of Models\", show_num_models=30)\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwAqPHcK0Tu9"
      },
      "source": [
        "### Battles Excluding Ties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "f0vh1TcQ0YBd",
        "outputId": "8f4fa525-6856-4de5-c44a-0c18e02e45c9"
      },
      "outputs": [],
      "source": [
        "visualize_battle_count(battles_no_ties, \"Battle Count for Each Combination of Models (without Ties)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqSGI-dA-3By"
      },
      "source": [
        "### Counting Ties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "cemVcoOMQEol",
        "outputId": "eb1ac2b6-2368-4926-fa5f-8460a89073a4"
      },
      "outputs": [],
      "source": [
        "visualize_battle_count(battles[battles['winner'].str.contains(\"tie\")], \"Tie Count for Each Combination of Models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgWNuDWy6jE6"
      },
      "source": [
        "## Inferred Language\n",
        "\n",
        "We also inferred the language for each conversation using `polyglot` package. This is just an estimate but will help guide future analysis.  The vast majority of conversations were in English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_lC0qLfKgmM9"
      },
      "outputs": [],
      "source": [
        "lang_count = battles[\"language\"].value_counts()\n",
        "lang_count = lang_count.drop(index=(\"unknown\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "P5-yrGGPIZ9y",
        "outputId": "55bd0d47-49bf-4aea-a325-7459dcd28283"
      },
      "outputs": [],
      "source": [
        "topk = 15\n",
        "fig = px.bar(lang_count.head(topk),\n",
        "             title=f\"Battle Counts for the Top {topk} Languages\",\n",
        "             text_auto=True, height=400, log_y=True)\n",
        "fig.update_layout(xaxis_title=\"Language\", yaxis_title=\"Count\", showlegend=False)\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-eu9nhUqUYB"
      },
      "source": [
        "## Number of Conversation Turns\n",
        "\n",
        "We also noticed that most counversations only have one turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "jaNaDmasqcnH",
        "outputId": "5bc8c7a1-abd2-4f02-9f99-5a472742dbd6"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(battles[\"turn\"],\n",
        "             title=f\"Number of Conversation Turns\",\n",
        "             text_auto=True, height=400, log_y=True)\n",
        "fig.update_layout(xaxis_title=\"Turns\", yaxis_title=\"Count\", showlegend=False)\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViV11W8l9NfL"
      },
      "source": [
        "## Pairwise Win Fractions\n",
        "\n",
        "Finally, we can also compute the pairwise win fractions. However, because each model can play as Model A and as Model B and win in both situations we need to compute the wins in both configurations divided by the number of pairings of each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tHZtIRD5JRHG"
      },
      "outputs": [],
      "source": [
        "def compute_pairwise_win_fraction(battles, max_num_models=30):\n",
        "    # Times each model wins as Model A\n",
        "    a_win_ptbl = pd.pivot_table(\n",
        "        battles[battles['winner'] == \"model_a\"],\n",
        "        index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0)\n",
        "\n",
        "    # Table counting times each model wins as Model B\n",
        "    b_win_ptbl = pd.pivot_table(\n",
        "        battles[battles['winner'] == \"model_b\"],\n",
        "        index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0)\n",
        "\n",
        "    # Table counting number of A-B pairs\n",
        "    num_battles_ptbl = pd.pivot_table(battles,\n",
        "        index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0)\n",
        "\n",
        "    # Computing the proportion of wins for each model as A and as B\n",
        "    # against all other models\n",
        "    row_beats_col_freq = (\n",
        "        (a_win_ptbl + b_win_ptbl.T) /\n",
        "        (num_battles_ptbl + num_battles_ptbl.T)\n",
        "    )\n",
        "\n",
        "    # Arrange ordering according to proprition of wins\n",
        "    prop_wins = row_beats_col_freq.mean(axis=1).sort_values(ascending=False)\n",
        "    prop_wins = prop_wins[:max_num_models]\n",
        "    model_names = list(prop_wins.keys())\n",
        "    row_beats_col = row_beats_col_freq.loc[model_names, model_names]\n",
        "    return row_beats_col\n",
        "\n",
        "def visualize_pairwise_win_fraction(battles, title, max_num_models=30):\n",
        "    row_beats_col = compute_pairwise_win_fraction(battles, max_num_models)\n",
        "    fig = px.imshow(row_beats_col, color_continuous_scale='RdBu',\n",
        "                    text_auto=\".2f\", title=title)\n",
        "    fig.update_layout(xaxis_title=\" Model B: Loser\",\n",
        "                  yaxis_title=\"Model A: Winner\",\n",
        "                  xaxis_side=\"top\", height=900, width=900,\n",
        "                  title_y=0.07, title_x=0.5)\n",
        "    fig.update_traces(hovertemplate=\n",
        "                  \"Model A: %{y}<br>Model B: %{x}<br>Fraction of A Wins: %{z}<extra></extra>\")\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "WhGcfNz-Luni",
        "outputId": "565e74e4-13b5-4ef5-9cab-5fe070c77245"
      },
      "outputs": [],
      "source": [
        "fig = visualize_pairwise_win_fraction(battles_no_ties,\n",
        "      title = \"Fraction of Model A Wins for All Non-tied A vs. B Battles\")\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq3NlxirIyb7"
      },
      "source": [
        "## Preliminary Ranking\n",
        "\n",
        "Using just the average win rate against all other models we can already compute an estimated leaderboard.\n",
        "However, this method may not be as scalable as the Elo rating system that we will use later because this method requires data from all model combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Y0RzCkEVIR9c",
        "outputId": "178cb5c9-cee2-48d9-95b5-4e8b65972533"
      },
      "outputs": [],
      "source": [
        "row_beats_col_freq = compute_pairwise_win_fraction(battles_no_ties)\n",
        "fig = px.bar(row_beats_col_freq.mean(axis=1).sort_values(ascending=False),\n",
        "             title=\"Average Win Rate Against All Other Models (Assuming Uniform Sampling and No Ties)\",\n",
        "             text_auto=\".2f\")\n",
        "fig.update_layout(yaxis_title=\"Average Win Rate\", xaxis_title=\"Model\",\n",
        "                  showlegend=False)\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_PYA7oVyaHO"
      },
      "source": [
        "#Elo Ratings\n",
        "\n",
        "The [Elo rating system ](https://en.wikipedia.org/wiki/Elo_rating_system)is a method for calculating the relative skill levels of players, which has been widely adopted in chess and other competitive games. The difference in the ratings between two players serves as a predictor of the outcome of a match. The Elo rating system works well for our case because we have multiple models and we run pairwise battles between them.\n",
        "In this section, we present different methods for calculating Elo ratings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLGc6DwxyvQc"
      },
      "source": [
        "### Compute Ratings\n",
        "We first use the online linear update algorithm to compute Elo ratings.\n",
        "We choose a small K-factor of 4 to make the Elo ratings more stable and less biased towards recent games."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hytEb0aXfcwm"
      },
      "outputs": [],
      "source": [
        "def compute_online_elo(battles, K=4, SCALE=400, BASE=10, INIT_RATING=1000):\n",
        "    rating = defaultdict(lambda: INIT_RATING)\n",
        "\n",
        "    for rd, model_a, model_b, winner in battles[['model_a', 'model_b', 'winner']].itertuples():\n",
        "        ra = rating[model_a]\n",
        "        rb = rating[model_b]\n",
        "        ea = 1 / (1 + BASE ** ((rb - ra) / SCALE))\n",
        "        eb = 1 / (1 + BASE ** ((ra - rb) / SCALE))\n",
        "        if winner == \"model_a\":\n",
        "            sa = 1\n",
        "        elif winner == \"model_b\":\n",
        "            sa = 0\n",
        "        elif winner == \"tie\" or winner == \"tie (bothbad)\":\n",
        "            sa = 0.5\n",
        "        else:\n",
        "            raise Exception(f\"unexpected vote {winner}\")\n",
        "        rating[model_a] += K * (sa - ea)\n",
        "        rating[model_b] += K * (1 - sa - eb)\n",
        "\n",
        "    # calibrate llama-13b to 800\n",
        "    delta = (800-rating[\"llama-13b\"])\n",
        "    for model in battles[\"model_a\"].unique():\n",
        "        rating[model] += delta\n",
        "\n",
        "    return rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "adJho2SYnLdw",
        "outputId": "fa47d204-b0c9-4809-a097-3cbfbd206b5f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Elo rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gemini-1.5-pro-api-0409-preview</td>\n",
              "      <td>1106.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gemini-1.5-pro-exp-0801</td>\n",
              "      <td>1074.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>chatgpt-4o-latest</td>\n",
              "      <td>1073.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gpt-3.5-turbo-0314</td>\n",
              "      <td>1051.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>bard-jan-24-gemini-pro</td>\n",
              "      <td>1041.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>chatglm3-6b</td>\n",
              "      <td>814.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>oasst-pythia-12b</td>\n",
              "      <td>812.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>llama-13b</td>\n",
              "      <td>800.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>fastchat-t5-3b</td>\n",
              "      <td>794.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>dolly-v2-12b</td>\n",
              "      <td>781.44</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>129 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                               Model  Elo rating\n",
              "1    gemini-1.5-pro-api-0409-preview     1106.87\n",
              "2            gemini-1.5-pro-exp-0801     1074.94\n",
              "3                  chatgpt-4o-latest     1073.74\n",
              "4                 gpt-3.5-turbo-0314     1051.02\n",
              "5             bard-jan-24-gemini-pro     1041.26\n",
              "..                               ...         ...\n",
              "125                      chatglm3-6b      814.55\n",
              "126                 oasst-pythia-12b      812.39\n",
              "127                        llama-13b      800.00\n",
              "128                   fastchat-t5-3b      794.37\n",
              "129                     dolly-v2-12b      781.44\n",
              "\n",
              "[129 rows x 2 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preety_print_model_ratings(ratings):\n",
        "    df = pd.DataFrame([\n",
        "        [n, ratings[n]] for n in ratings.keys()\n",
        "    ], columns=[\"Model\", \"Elo rating\"]).sort_values(\"Elo rating\", ascending=False).reset_index(drop=True)\n",
        "    # df[\"Elo rating\"] = (df[\"Elo rating\"] + 0.5).astype(int)\n",
        "    df.index = df.index + 1\n",
        "    return df\n",
        "\n",
        "online_elo_ratings = compute_online_elo(battles)\n",
        "preety_print_model_ratings(online_elo_ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlaohCVFnh2g"
      },
      "source": [
        "However, even with a small K-factor, we still found this online update algorithm to be unstable.\n",
        "\n",
        "To demonstrate it, we recompute Elo rating by using the reversed game order and observe significant difference due to online update of Elo which biases the recent games."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "c0KvFVr-nR2Q",
        "outputId": "c587f305-2d5e-4047-a380-f8416ab178a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Elo rating</th>\n",
              "      <th>Elo rating with reverse order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gemini-1.5-pro-api-0409-preview</td>\n",
              "      <td>1107</td>\n",
              "      <td>1112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gemini-1.5-pro-exp-0801</td>\n",
              "      <td>1075</td>\n",
              "      <td>1086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>chatgpt-4o-latest</td>\n",
              "      <td>1074</td>\n",
              "      <td>1151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gpt-3.5-turbo-0314</td>\n",
              "      <td>1051</td>\n",
              "      <td>1162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>bard-jan-24-gemini-pro</td>\n",
              "      <td>1041</td>\n",
              "      <td>1155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>chatglm3-6b</td>\n",
              "      <td>815</td>\n",
              "      <td>893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>oasst-pythia-12b</td>\n",
              "      <td>812</td>\n",
              "      <td>907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>llama-13b</td>\n",
              "      <td>800</td>\n",
              "      <td>800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>fastchat-t5-3b</td>\n",
              "      <td>794</td>\n",
              "      <td>863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>dolly-v2-12b</td>\n",
              "      <td>781</td>\n",
              "      <td>833</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>129 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                               Model  Elo rating  \\\n",
              "1    gemini-1.5-pro-api-0409-preview        1107   \n",
              "2            gemini-1.5-pro-exp-0801        1075   \n",
              "3                  chatgpt-4o-latest        1074   \n",
              "4                 gpt-3.5-turbo-0314        1051   \n",
              "5             bard-jan-24-gemini-pro        1041   \n",
              "..                               ...         ...   \n",
              "125                      chatglm3-6b         815   \n",
              "126                 oasst-pythia-12b         812   \n",
              "127                        llama-13b         800   \n",
              "128                   fastchat-t5-3b         794   \n",
              "129                     dolly-v2-12b         781   \n",
              "\n",
              "     Elo rating with reverse order  \n",
              "1                             1112  \n",
              "2                             1086  \n",
              "3                             1151  \n",
              "4                             1162  \n",
              "5                             1155  \n",
              "..                             ...  \n",
              "125                            893  \n",
              "126                            907  \n",
              "127                            800  \n",
              "128                            863  \n",
              "129                            833  \n",
              "\n",
              "[129 rows x 3 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preety_print_two_ratings(ratings_1, ratings_2, column_names):\n",
        "    df = pd.DataFrame([\n",
        "        [n, ratings_1[n], ratings_2[n]] for n in ratings_1.keys()\n",
        "    ], columns=[\"Model\", column_names[0], column_names[1]]).sort_values(column_names[0], ascending=False).reset_index(drop=True)\n",
        "    df[column_names[0]] = (df[column_names[0]] + 0.5).astype(int)\n",
        "    df[column_names[1]] = (df[column_names[1]] + 0.5).astype(int)\n",
        "    df.index = df.index + 1\n",
        "    return df\n",
        "\n",
        "elo_mle_ratings_reverse = compute_online_elo(battles.iloc[::-1])\n",
        "preety_print_two_ratings(online_elo_ratings,\n",
        "                         elo_mle_ratings_reverse,\n",
        "                         column_names=[\"Elo rating\", \"Elo rating with reverse order\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbTdhkLQp113"
      },
      "source": [
        "\n",
        "### Maximum Likelihood Estimation with [Bradley-Terry model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model)\n",
        "\n",
        "In the context of LLM evaluation, models can be assumed to be static. In this case, we can directly fit the ratings by maximum likelihood estimation method (aka Bradley-Terry model), which produce significantly stable ratings. Here we provide an implementation with logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mSizG3Pzglte"
      },
      "outputs": [],
      "source": [
        "def compute_mle_elo(\n",
        "    df, SCALE=400, BASE=10, INIT_RATING=1000, sample_weight=None\n",
        "):\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    ptbl_a_win = pd.pivot_table(\n",
        "        df[df[\"winner\"] == \"model_a\"],\n",
        "        index=\"model_a\",\n",
        "        columns=\"model_b\",\n",
        "        aggfunc=\"size\",\n",
        "        fill_value=0,\n",
        "    )\n",
        "    # if no tie, create a zero matrix\n",
        "    if sum(df[\"winner\"].isin([\"tie\", \"tie (bothbad)\"])) == 0:\n",
        "        ptbl_tie = pd.DataFrame(0, index=ptbl_a_win.index, columns=ptbl_a_win.columns)\n",
        "    else:\n",
        "        ptbl_tie = pd.pivot_table(\n",
        "            df[df[\"winner\"].isin([\"tie\", \"tie (bothbad)\"])],\n",
        "            index=\"model_a\",\n",
        "            columns=\"model_b\",\n",
        "            aggfunc=\"size\",\n",
        "            fill_value=0,\n",
        "        )\n",
        "        ptbl_tie = ptbl_tie + ptbl_tie.T\n",
        "\n",
        "    ptbl_b_win = pd.pivot_table(\n",
        "        df[df[\"winner\"] == \"model_b\"],\n",
        "        index=\"model_a\",\n",
        "        columns=\"model_b\",\n",
        "        aggfunc=\"size\",\n",
        "        fill_value=0,\n",
        "    )\n",
        "    ptbl_win = ptbl_a_win * 2 + ptbl_b_win.T * 2 + ptbl_tie\n",
        "\n",
        "    models = pd.Series(np.arange(len(ptbl_win.index)), index=ptbl_win.index)\n",
        "\n",
        "    p = len(models)\n",
        "    X = np.zeros([p * (p - 1) * 2, p])\n",
        "    Y = np.zeros(p * (p - 1) * 2)\n",
        "\n",
        "    cur_row = 0\n",
        "    sample_weights = []\n",
        "    for m_a in ptbl_win.index:\n",
        "        for m_b in ptbl_win.columns:\n",
        "            if m_a == m_b:\n",
        "                continue\n",
        "            # if nan skip\n",
        "            if math.isnan(ptbl_win.loc[m_a, m_b]) or math.isnan(ptbl_win.loc[m_b, m_a]):\n",
        "                continue\n",
        "            X[cur_row, models[m_a]] = +math.log(BASE)\n",
        "            X[cur_row, models[m_b]] = -math.log(BASE)\n",
        "            Y[cur_row] = 1.0\n",
        "            sample_weights.append(ptbl_win.loc[m_a, m_b])\n",
        "\n",
        "            X[cur_row + 1, models[m_a]] = math.log(BASE)\n",
        "            X[cur_row + 1, models[m_b]] = -math.log(BASE)\n",
        "            Y[cur_row + 1] = 0.0\n",
        "            sample_weights.append(ptbl_win.loc[m_b, m_a])\n",
        "            cur_row += 2\n",
        "    X = X[:cur_row]\n",
        "    Y = Y[:cur_row]\n",
        "\n",
        "    lr = LogisticRegression(fit_intercept=False, penalty=None, tol=1e-6)\n",
        "    lr.fit(X, Y, sample_weight=sample_weights)\n",
        "    elo_scores = SCALE * lr.coef_[0] + INIT_RATING\n",
        "    if \"mixtral-8x7b-instruct-v0.1\" in models.index:\n",
        "        elo_scores += 1114 - elo_scores[models[\"mixtral-8x7b-instruct-v0.1\"]]\n",
        "    return pd.Series(elo_scores, index=models.index).sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3v8wc_oCmtmW",
        "outputId": "fab47477-7560-470c-a4e0-2a21cadfc88b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Elo rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>chatgpt-4o-latest</td>\n",
              "      <td>1315.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gemini-1.5-pro-exp-0801</td>\n",
              "      <td>1298.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gpt-4o-2024-05-13</td>\n",
              "      <td>1285.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gpt-4o-mini-2024-07-18</td>\n",
              "      <td>1274.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>claude-3-5-sonnet-20240620</td>\n",
              "      <td>1270.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>chatglm-6b</td>\n",
              "      <td>879.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>fastchat-t5-3b</td>\n",
              "      <td>868.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>stablelm-tuned-alpha-7b</td>\n",
              "      <td>839.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>dolly-v2-12b</td>\n",
              "      <td>822.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>llama-13b</td>\n",
              "      <td>798.82</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>129 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                          Model  Elo rating\n",
              "1             chatgpt-4o-latest     1315.13\n",
              "2       gemini-1.5-pro-exp-0801     1298.44\n",
              "3             gpt-4o-2024-05-13     1285.83\n",
              "4        gpt-4o-mini-2024-07-18     1274.87\n",
              "5    claude-3-5-sonnet-20240620     1270.98\n",
              "..                          ...         ...\n",
              "125                  chatglm-6b      879.08\n",
              "126              fastchat-t5-3b      868.37\n",
              "127     stablelm-tuned-alpha-7b      839.74\n",
              "128                dolly-v2-12b      822.12\n",
              "129                   llama-13b      798.82\n",
              "\n",
              "[129 rows x 2 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "elo_mle_ratings = compute_mle_elo(battles)\n",
        "preety_print_model_ratings(elo_mle_ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92pAnooyzBCG"
      },
      "source": [
        "### Compute Bootstrap Confidence Interavals for MLE Elo Scores\n",
        "\n",
        "We can further use bootstrap to estimate the confidence intervals as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K9Plp9KhAu2n"
      },
      "outputs": [],
      "source": [
        "def get_bootstrap_result(battles, func_compute_elo, num_round):\n",
        "    rows = []\n",
        "    for i in tqdm(range(num_round), desc=\"bootstrap\"):\n",
        "        rows.append(func_compute_elo(battles.sample(frac=1.0, replace=True)))\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df[df.median().sort_values(ascending=False).index]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZbrbbbRiV95",
        "outputId": "dfe4e731-44ca-49ed-b4bb-2f6ab508ce43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "bootstrap: 100%|██████████| 100/100 [20:16<00:00, 12.17s/it]\n"
          ]
        }
      ],
      "source": [
        "BOOTSTRAP_ROUNDS = 100\n",
        "\n",
        "np.random.seed(42)\n",
        "bootstrap_elo_lu = get_bootstrap_result(battles, compute_mle_elo, BOOTSTRAP_ROUNDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from faster import get_bootstrap_result as get_bootstrap_result_fast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "BOOTSTRAP_ROUNDS = 100\n",
        "\n",
        "np.random.seed(42)\n",
        "bootstrap_elo_lu_fast = get_bootstrap_result_fast(battles, BOOTSTRAP_ROUNDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean diff: -0.076558166808036\n",
            "mean abs diff: 0.3204303657250496\n"
          ]
        }
      ],
      "source": [
        "original_bootstrap_mean = bootstrap_elo_lu.values.mean(axis=0)\n",
        "fast_bootstrap_mean = bootstrap_elo_lu_fast.values.mean(axis=0)\n",
        "diff = original_bootstrap_mean - fast_bootstrap_mean\n",
        "print(f'mean diff: {np.mean(diff)}')\n",
        "print(f'mean abs diff: {np.mean(np.abs(diff))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "njg7wkbuZbHx",
        "outputId": "acfd0f2e-6fa4-449f-a724-0dd588947d93"
      },
      "outputs": [],
      "source": [
        "def visualize_bootstrap_scores(df, title):\n",
        "    bars = pd.DataFrame(dict(\n",
        "        lower = df.quantile(.025),\n",
        "        rating = df.quantile(.5),\n",
        "        upper = df.quantile(.975))).reset_index(names=\"model\").sort_values(\"rating\", ascending=False)\n",
        "    bars['error_y'] = bars['upper'] - bars[\"rating\"]\n",
        "    bars['error_y_minus'] = bars['rating'] - bars[\"lower\"]\n",
        "    bars['rating_rounded'] = np.round(bars['rating'], 2)\n",
        "    fig = px.scatter(bars, x=\"model\", y=\"rating\", error_y=\"error_y\",\n",
        "                     error_y_minus=\"error_y_minus\", text=\"rating_rounded\",\n",
        "                     title=title)\n",
        "    fig.update_layout(xaxis_title=\"Model\", yaxis_title=\"Rating\",\n",
        "                      height=600)\n",
        "    return fig\n",
        "\n",
        "fig = visualize_bootstrap_scores(bootstrap_elo_lu, \"Bootstrap of MLE Elo Rating Estimates\")\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_CpbkGEbhrK"
      },
      "source": [
        "We previously apply bootstrapping on the online Elo to obtain stabler ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mukqgshMarFi",
        "outputId": "f81b586a-e7b6-4c5e-874f-835e037ec1fd"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "bootstrap_online_elo = get_bootstrap_result(battles, compute_online_elo, BOOTSTRAP_ROUNDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW4o1eiYeIYZ"
      },
      "source": [
        "We can see the bootstrapping medians obtained by both methods are similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "HdZrGr4IcWCl",
        "outputId": "987d428e-ba2b-44cd-a159-75d8bf2443e7"
      },
      "outputs": [],
      "source": [
        "preety_print_two_ratings(bootstrap_elo_lu.quantile(.5),\n",
        "                         bootstrap_online_elo.quantile(.5),\n",
        "                         column_names=[\"Bootstrap Median of MLE Elo\", \"Bootstrap Median of Online Elo\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXyD6TILeCaJ"
      },
      "source": [
        "However, online Elo's confidence intervals are significantly larger than the MLE Elo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "34Qp7yy6bWgN",
        "outputId": "c255c09d-cda1-499c-e743-35f3c36b5557"
      },
      "outputs": [],
      "source": [
        "fig = visualize_bootstrap_scores(bootstrap_online_elo, \"Bootstrap of Online Elo Rating Estimates\")\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsk4LxTWTxRx"
      },
      "source": [
        "### Predict Win Rates\n",
        "Utilizing Elo ratings allows us to predict win probabilities. By comparing the predicted win rates with the actual win rates, we can gain insight into the accuracy and quality of the Elo rating system.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0ZH43bEeVqY"
      },
      "outputs": [],
      "source": [
        "def predict_win_rate(elo_ratings, SCALE=400, BASE=10, INIT_RATING=1000):\n",
        "    names = sorted(list(elo_ratings.keys()))\n",
        "    wins = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "    for a in names:\n",
        "        for b in names:\n",
        "            ea = 1 / (1 + BASE ** ((elo_ratings[b] - elo_ratings[a]) / SCALE))\n",
        "            wins[a][b] = ea\n",
        "            wins[b][a] = 1 - ea\n",
        "\n",
        "    data = {\n",
        "        a: [wins[a][b] if a != b else np.NAN for b in names]\n",
        "        for a in names\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data, index=names)\n",
        "    df.index.name = \"model_a\"\n",
        "    df.columns.name = \"model_b\"\n",
        "    return df.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "p0FkQPRxQyi6",
        "outputId": "a5797fbd-4148-4420-84d9-e4e35acc83ec"
      },
      "outputs": [],
      "source": [
        "win_rate = predict_win_rate(dict(bootstrap_elo_lu.quantile(0.5)))\n",
        "ordered_models = win_rate.mean(axis=1).sort_values(ascending=False).index\n",
        "ordered_models = ordered_models[:30]\n",
        "fig = px.imshow(win_rate.loc[ordered_models, ordered_models],\n",
        "                color_continuous_scale='RdBu', text_auto=\".2f\",\n",
        "                title=\"Predicted Win Rate Using Elo Ratings for Model A in an A vs. B Battle\")\n",
        "fig.update_layout(xaxis_title=\"Model B\",\n",
        "                  yaxis_title=\"Model A\",\n",
        "                  xaxis_side=\"top\", height=900, width=900,\n",
        "                  title_y=0.07, title_x=0.5)\n",
        "fig.update_traces(hovertemplate=\n",
        "                  \"Model A: %{y}<br>Model B: %{x}<br>Win Rate: %{z}<extra></extra>\")\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lBa9pdtzMA3"
      },
      "source": [
        "### Compute Bootstrap Confidence Intervals Assuming Uniform Sampling\n",
        "\n",
        "We also study how the ratings will change if we only sample an equal number of battles for each model pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTwWk15zZSTh"
      },
      "outputs": [],
      "source": [
        "def sample_battle_even(battles, n_per_battle):\n",
        "    groups = battles.groupby([\"model_a\", \"model_b\"], as_index=False)\n",
        "    resampled = (groups\n",
        "                 .apply(lambda grp: grp.sample(n_per_battle, replace=True))\n",
        "                 .reset_index(drop=True))\n",
        "    return resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        },
        "id": "GM_zlH-yZRhz",
        "outputId": "5db42f6c-face-49f8-beec-7b4c4ea81abb"
      },
      "outputs": [],
      "source": [
        "num_samples = 50\n",
        "battles_even = sample_battle_even(battles, num_samples)\n",
        "pd.pivot_table(battles_even, index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hkd4dxRR-DI"
      },
      "outputs": [],
      "source": [
        "# Sampling Battles Evenly\n",
        "def get_bootstrap_even_sample(battles, n_per_battle, func_compute_elo, num_round=BOOTSTRAP_ROUNDS):\n",
        "    rows = []\n",
        "    for n in tqdm(range(num_round), desc=\"sampling battles evenly\"):\n",
        "        resampled = sample_battle_even(battles, n_per_battle)\n",
        "        rows.append(func_compute_elo(resampled))\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df[df.median().sort_values(ascending=False).index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHfd6btNXYKt",
        "outputId": "a5749104-6d8e-4a05-a148-20903fe672d9"
      },
      "outputs": [],
      "source": [
        "print(\"number of samples per battle pair:\", num_samples)\n",
        "bootstrap_even_lu = get_bootstrap_even_sample(battles, num_samples, compute_mle_elo, num_round=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "JJWnGH9c8Q3g",
        "outputId": "239f9390-6a29-48ce-bcf4-8fb14700735f"
      },
      "outputs": [],
      "source": [
        "fig = visualize_bootstrap_scores(bootstrap_even_lu, f\"Bootstrap of MLE Elo Estimates - Even sample\")\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAI96WZJ9fxQ"
      },
      "source": [
        "# Language-specific Leaderboards\n",
        "We present two language-specific leaderboards, by isolating the chat data into two subsets based on the language: (1) English-only and (2) Non-English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a9ZUqPC9l0-"
      },
      "source": [
        "## English-only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "YBnS0fCL9wE2",
        "outputId": "896f179b-5552-4268-bcfb-dd45021f5fda"
      },
      "outputs": [],
      "source": [
        "english_only_battles = battles[battles[\"language\"] == \"English\"]\n",
        "elo_ratings = compute_mle_elo(english_only_battles)\n",
        "pd.DataFrame(elo_ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn7OZDdh9noH"
      },
      "source": [
        "## Non-English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "mxIKKbif9wuA",
        "outputId": "3dcf6f00-9096-4fe6-d933-535603869cd2"
      },
      "outputs": [],
      "source": [
        "non_english_battles = battles[battles[\"language\"] != \"English\"]\n",
        "elo_ratings = compute_mle_elo(non_english_battles)\n",
        "pd.DataFrame(elo_ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGwWvngLONeS"
      },
      "source": [
        "# Links\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi-X5I7UPkxU"
      },
      "source": [
        "\n",
        "Some good resources to learn more about Elo rating systems:\n",
        "- Elo rating system https://en.wikipedia.org/wiki/Elo_rating_system\n",
        "- Bradley-Terry model https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model\n",
        "- An introduction video https://www.youtube.com/watch?v=AsYfbmp0To0\n",
        "- A FiveThirtyEight article https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP35mjnHfpfN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
